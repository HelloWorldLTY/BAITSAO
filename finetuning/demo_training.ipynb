{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import lightning as L\n",
    "\n",
    "import os, sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gzip\n",
    "import sklearn.model_selection\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grountruth_score = pd.read_csv(\"/gpfs/gibbs/pi/zhao/tl688/synergy_prediction/labels_synergy_value.csv\")\n",
    "df_grountruth_score.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/gpfs/gibbs/pi/zhao/tl688/cpsc_finalproject/genept_data/GenePT/ensem_emb_deepsynergycellline.pickle\", 'rb') as f:\n",
    "    cellline_name_getembedding = pickle.load(f)\n",
    "with open(\"/gpfs/gibbs/pi/zhao/tl688/cpsc_finalproject/genept_data/GenePT/ensem_emb_deepsynergydrug.pickle\", 'rb') as f:\n",
    "    drug_name_getembedding = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fold = 0\n",
    "train_index = df_grountruth_score[df_grountruth_score['fold'] != test_fold].index\n",
    "\n",
    "train_list = {}\n",
    "for item in train_index:\n",
    "    d1, d2, cl = df_grountruth_score.loc[item]['Unnamed: 0'].split('_')\n",
    "    value_list = (drug_name_getembedding[d1] + drug_name_getembedding[d2]) / 2 \n",
    "    value_list = np.hstack([value_list,cellline_name_getembedding[cl]])\n",
    "    train_list[item] = value_list\n",
    "\n",
    "X_train = np.array(list(train_list.values()))\n",
    "y_train = df_grountruth_score.loc[train_index]['synergy'].values\n",
    "y_train = (y_train > 30)*1\n",
    "\n",
    "# X_train, X_val, y_train, y_val = sklearn.model_selection.train_test_split(X_train, y_train, random_state=2023)\n",
    "\n",
    "layers = [10240,4096,1] \n",
    "epochs = 1000 \n",
    "act_func = 'GELU'\n",
    "dropout = 0.5\n",
    "input_dropout = 0.2\n",
    "eta = 1e-5 \n",
    "norm = 'tanh' \n",
    "drug_num_dim = 16\n",
    "\n",
    "df_test = df_grountruth_score[df_grountruth_score['fold'] == test_fold]\n",
    "\n",
    "test_list = {}\n",
    "for item in df_test.index.values:\n",
    "    d1, d2, cl = df_grountruth_score.loc[item]['Unnamed: 0'].split('_')\n",
    "    value_list = (drug_name_getembedding[d1] + drug_name_getembedding[d2]) / 2 \n",
    "    value_list = np.hstack([value_list,cellline_name_getembedding[cl]])\n",
    "    test_list[item] = value_list\n",
    "\n",
    "X_test = np.array(list(test_list.values()))\n",
    "y_test = df_grountruth_score.loc[df_test.index.values]['synergy'].values\n",
    "y_test = (y_test > 30)*1\n",
    "\n",
    "# X_tr, X_test, y_tr, y_test = sklearn.model_selection.train_test_split(X_train, y_train, test_size=0.1, random_state = 2023)\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = sklearn.model_selection.train_test_split(X_train, y_train, random_state = 2023)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.l1 = nn.Sequential(nn.Linear(X_tr.shape[1], layers[0]), \n",
    "                                nn.BatchNorm1d(layers[0], momentum=0.1),\n",
    "                                nn.ReLU(), \n",
    "                                nn.Dropout(input_dropout),\n",
    "                                nn.Linear(layers[0], layers[1]),\n",
    "                                nn.BatchNorm1d(layers[1], momentum=0.1),\n",
    "                                nn.ReLU(), \n",
    "                                nn.Dropout(input_dropout),\n",
    "                                nn.Linear(layers[1], layers[2]),\n",
    "                                nn.Sigmoid()\n",
    "                               )\n",
    "\n",
    "        \n",
    "\n",
    "        self.drug_num_emb = nn.Embedding(10, drug_num_dim)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.l1(x)\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "\n",
    "        z = self.l1(x)\n",
    "\n",
    "        return z\n",
    "\n",
    "class LitAutoEncoder(L.LightningModule):\n",
    "    def __init__(self, encoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "#         self.encoder.apply(init_weights)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        x, y = batch\n",
    "#         print(self.encoder.drug_num_emb(x[:,-1].long()).shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x_new = x\n",
    "#         x_new = torch.cat([x[:,:-1], self.encoder.drug_num_emb(x[:,-1].long())], axis=1)\n",
    "        z = self.encoder(x_new)\n",
    "        loss = F.binary_cross_entropy(z,y.view(x.size(0), -1))\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # this is the validation loop\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x_new = x\n",
    "#         x_new = torch.cat([x[:,:-1], self.encoder.drug_num_emb(x[:,-1].long())], axis=1)\n",
    "        z = self.encoder(x_new)\n",
    "        val_loss = F.binary_cross_entropy(z,y.view(x.size(0), -1))\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "        return val_loss\n",
    "\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # this is the test loop\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x_new = x\n",
    "#         x_new = torch.cat([x[:,:-1], self.encoder.drug_num_emb(x[:,-1].long())], axis=1)\n",
    "        z = self.encoder(x_new)\n",
    "        test_loss = F.binary_cross_entropy(z,y.view(x.size(0), -1))\n",
    "        self.log(\"test_loss\", test_loss)\n",
    "        return test_loss\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "            optimizer = torch.optim.Adam(\n",
    "                params=self.parameters(), \n",
    "                lr=eta,\n",
    "#                 weight_decay=1e-4\n",
    "            )\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                patience=10,\n",
    "                verbose=True\n",
    "            )\n",
    "            return {\n",
    "               'optimizer': optimizer,\n",
    "               'lr_scheduler': scheduler, # Changed scheduler to lr_scheduler\n",
    "               'monitor': 'val_loss'\n",
    "           }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_val, X_train, X_test, y_tr, y_val, y_train, y_test =torch.FloatTensor(X_tr),torch.FloatTensor(X_val),torch.FloatTensor(X_train),torch.FloatTensor(X_test),torch.FloatTensor(y_tr), torch.FloatTensor(y_val), torch.FloatTensor(y_train), torch.FloatTensor(y_test)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_tr, y_tr)\n",
    "valid_dataset = torch.utils.data.TensorDataset(X_val, y_val)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "\n",
    "# layers = [10240,4096,1] \n",
    "# epochs = 1000 \n",
    "# act_func = 'GELU'\n",
    "# dropout = 0.5 \n",
    "# input_dropout = 0.2\n",
    "# eta = 1e-4 \n",
    "# norm = 'tanh' \n",
    "# drug_num_dim = 16\n",
    "\n",
    "model = LitAutoEncoder(Encoder())\n",
    "\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "model\n",
    "\n",
    "model.encoder.l1[0]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, num_workers=5, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=1024, num_workers=5)\n",
    "\n",
    "# train with both splits\n",
    "trainer = L.Trainer(callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=100)], max_epochs=1000)\n",
    "trainer.fit(model, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred = model.encoder.predict(X_test).detach()\n",
    "\n",
    "import scipy.stats\n",
    "import sklearn.metrics\n",
    "\n",
    "print(\"We use the fold with number:\", test_fold)\n",
    "print(sklearn.metrics.roc_auc_score(y_test, y_pred.t()[0].cpu().numpy()), sklearn.metrics.accuracy_score(y_test, (y_pred.t()[0].cpu().numpy()>0.5)*1))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
